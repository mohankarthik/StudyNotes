{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "SGD is an alternative that takes small chunks of the data and trains the system based on the small data set instead of the entire data set. But we do this many times and each time, we take a random subset of the training data and feed it into the system.\n",
    "![SGD](img/SGD.png)\n",
    "\n",
    "## Momentum\n",
    "Instead of taking the derivative of each subset of data separately, we take a running average of the training error so far, this helps us optimize our path to the convergence.\n",
    "![SGD Momentum](img/SGDMomentum.png)\n",
    "\n",
    "## Learning rate decay\n",
    "Make the steps smaller and smaller as the time goes on, it can be lowered as exponential of the time, or by decreasing the learning rate after it reaches a plateau\n",
    "\n",
    "## Parameters\n",
    "SGD has multiple parameters that can be tweaked\n",
    "* Initial learning rate = when things go wrong, reduce the learning rate\n",
    "* Learning rate decay = make it learn slower as time goes\n",
    "* Momentum = Averaging across how many iterations\n",
    "* Batch size = ahem\n",
    "* Weight initialization (mean and std dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Minibatch              Time                 Loss           Train accuracy    Validation accuracy \n",
      "         1          0.21798300000045856    0.585846722126      1.08244788647          1.33637       \n",
      "   [ 0.18243623]       [ 0.59682423]    \n",
      "        213          21.75861900000018     0.460385099405      0.935655421806         0.762129      \n",
      "   [ 0.32074052]       [ 0.72076029]    \n",
      "        425          21.779513999999836    0.265855848051      0.71992004972          0.819935      \n",
      "   [ 0.40400708]       [ 0.00069777]    \n",
      "        637          21.678541999999652    0.272418653206      0.729807570577         0.828552      \n",
      "   [ 0.41142675]       [ 0.32968023]    \n",
      "        849          21.94988900000044     0.273084905139      0.73074667212          0.829845      \n",
      "   [ 0.41240123]       [ 0.34814748]    \n",
      "        1061         22.031189999999697    0.271641644807      0.728752631624         0.830217      \n",
      "   [ 0.41298494]       [ 0.20582323]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-cfeba8551caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;31m# Script to execute the main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-cfeba8551caf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m                 (L_DECAY_STEP * NUM_STEPS), L_DECAY_RATE)\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mmyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#myNet.Print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-cfeba8551caf>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, num_steps, train_data, train_labels, cv_data, cv_labels, test_data, test_labels)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Run the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 _, l, predictions, summary_str = session.run([self.opt, self.loss, self.train, self.summary],\n\u001b[0;32m--> 187\u001b[0;31m                                                 {self.tf_x : data, self.tf_y : labels})\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Write logs for each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Some helper functions\n",
    "def accuracy(predictions, labels):\n",
    "    return np.linalg.norm(predictions - labels)\n",
    "\n",
    "# Let's define a simple NN\n",
    "class NeuralNet:\n",
    "    ########### PRIVATE METHODS ###################\n",
    "    # Geneates a fully connected model from the input, weights and biases\n",
    "    # keeping the number of layers and the dropout probability in mind\n",
    "    def __genModel(self, x, w, b, num_layers, keep_prob):\n",
    "        # Local variable activations\n",
    "        a = dict()\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            return tf.nn.bias_add(tf.matmul(x, w[0]), b[0])\n",
    "        else:\n",
    "            a[0] = tf.nn.dropout(tf.nn.relu(tf.nn.bias_add(tf.matmul(x, w[0]), b[0])), keep_prob)\n",
    "            for i in np.arange(1, num_layers):\n",
    "                a[i] = tf.nn.dropout(tf.nn.relu(tf.nn.bias_add(tf.matmul(a[i-1], w[i]), b[i])), keep_prob)\n",
    "            i = num_layers\n",
    "            return tf.nn.bias_add(tf.matmul(a[i-1], w[i]), b[i])\n",
    "        \n",
    "    ########### PUBLIC METHODS ###################\n",
    "    # Constructor\n",
    "    def __init__(self, batch_size, x_size, y_size):\n",
    "        # Store the values\n",
    "        self.batch_size = batch_size\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        \n",
    "        # Initialize the graph\n",
    "        self.graph = tf.Graph()\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds a configurable number of layers into the graph\n",
    "    num_layers: Number of fully connected layers to be added\n",
    "    w_sd: The allowed std-dev of the weights\n",
    "    num_nodes: A np array containing the number of nodes in each layer,\n",
    "        so num_nodes must be a list of num_layers length\n",
    "    keep_prob: The dropouts keep prob\n",
    "    reg_const: The L2 norm constant\n",
    "    init_lrate: Initial learning rate\n",
    "    lr_decay_step: Number of steps after which learning rate should decay\n",
    "    lr_decay_rate: The rate at which the learning rate should decay\n",
    "    \"\"\"\n",
    "    def Model(self, num_layers, w_sd, num_nodes, keep_prob, reg_const, init_lrate, lr_decay_step, lr_decay_rate):\n",
    "        # Preconditions\n",
    "        assert num_layers == num_nodes.shape[0]\n",
    "        \n",
    "        # Store the values\n",
    "        self.num_layers = num_layers\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Initialize the w and b & activation variables\n",
    "        self.tf_w = dict()\n",
    "        self.tf_b = dict()\n",
    "        self.tb_w = dict()\n",
    "        self.tb_b = dict()\n",
    "        \n",
    "        # Form the weights and biases\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope(\"input\") as scope:\n",
    "                self.tf_x = tf.placeholder(tf.float32, shape=(self.batch_size, self.x_size))\n",
    "                self.tf_y = tf.placeholder(tf.float32, shape=(self.batch_size, self.y_size))\n",
    "                \n",
    "            # If there are no layers, connect input to output directly\n",
    "            if num_layers == 0:\n",
    "                with tf.name_scope(\"output_layer\") as scope:\n",
    "                    self.tf_w[0] = tf.Variable(tf.truncated_normal([self.x_size, self.y_size], stddev=w_sd))\n",
    "                    self.tf_b[0] = tf.Variable(tf.zeros([self.y_size]))\n",
    "            else:\n",
    "                # Add the first layer\n",
    "                with tf.name_scope(\"hidden_layer_0\") as scope:\n",
    "                    self.tf_w[0] = tf.Variable(tf.truncated_normal([self.x_size, num_nodes[0]], stddev=w_sd))\n",
    "                    self.tf_b[0] = tf.Variable(tf.zeros([num_nodes[0]]))\n",
    "\n",
    "                # Add the intermediate layers\n",
    "                for i in np.arange(1, num_layers):\n",
    "                    with tf.name_scope(\"hidden_layer_\"+str(i)) as scope:\n",
    "                        self.tf_w[i] = tf.Variable(tf.truncated_normal([num_nodes[i-1], num_nodes[i]], stddev=w_sd))\n",
    "                        self.tf_b[i] = tf.Variable(tf.zeros([num_nodes[i]]))\n",
    "\n",
    "                # Connect to the output layer\n",
    "                i = num_layers\n",
    "                with tf.name_scope(\"output_layer\") as scope:\n",
    "                    self.tf_w[num_layers] = tf.Variable(tf.truncated_normal([num_nodes[i-1], self.y_size], stddev=w_sd))\n",
    "                    self.tf_b[num_layers] = tf.Variable(tf.zeros([self.y_size]))\n",
    "            \n",
    "            # Add summary ops to collect data\n",
    "            for i in np.arange(num_layers + 1):\n",
    "                self.tb_w[i] = tf.histogram_summary(\"weights\"+str(i), self.tf_w[i])\n",
    "                self.tb_b[i] = tf.histogram_summary(\"biases\"+str(i), self.tf_b[i])\n",
    "            \n",
    "            # Model the training, evaluation, cross validation and testing\n",
    "            self.logits = self.__genModel(self.tf_x, self.tf_w, self.tf_b, self.num_layers, keep_prob)\n",
    "            self.train = self.__genModel(self.tf_x, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            \n",
    "            # Implement the loss function\n",
    "            with tf.name_scope(\"cost_function\") as scope:\n",
    "                # Loss\n",
    "                self.loss = tf.reduce_mean(tf.nn.l2_loss(self.logits - self.tf_y))\n",
    "\n",
    "                # L2 regularization\n",
    "                reg = 0\n",
    "                for i in np.arange(self.num_layers + 1):\n",
    "                    reg += tf.nn.l2_loss(self.tf_w[i]) + tf.nn.l2_loss(self.tf_b[i])\n",
    "                reg *= reg_const\n",
    "\n",
    "                # Add the regularization to the loss\n",
    "                self.loss = tf.reduce_mean(self.loss + reg)\n",
    "\n",
    "                # Create a summary to monitor the cost function\n",
    "                tf.scalar_summary(\"cost_function\", self.loss)\n",
    "\n",
    "            # Create the global step variable\n",
    "            self.gstep = 0\n",
    "\n",
    "            # Implement the optimizer\n",
    "            with tf.name_scope(\"back_prop\") as scope:\n",
    "                # Optimizer with a decaying learning rate\n",
    "                self.lrate = tf.train.exponential_decay(init_lrate, self.gstep, \n",
    "                                                        lr_decay_step, lr_decay_rate, staircase=True)\n",
    "                self.opt = tf.train.GradientDescentOptimizer(self.lrate).minimize(self.loss)\n",
    "\n",
    "                # Create a summary to monitor the cost function\n",
    "                tf.scalar_summary(\"learning_rate\", self.lrate)\n",
    "            \n",
    "            # Merge all the summaries and start the summary writer\n",
    "            self.summary = tf.merge_all_summaries()\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the model. This should be invoked only after the model is formed\n",
    "    \n",
    "    \"\"\"\n",
    "    def Train(self, num_steps, train_data, train_labels, cv_data, cv_labels, test_data, test_labels):\n",
    "        with self.graph.as_default():\n",
    "            # Add the test and cross validation data sets\n",
    "            self.tf_cv = tf.constant(cv_data)\n",
    "            self.tf_tst = tf.constant(test_data)\n",
    "\n",
    "            # Set up the evaluations for the CV and test sets\n",
    "            self.cv = self.__genModel(self.tf_cv, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            self.tst = self.__genModel(self.tf_tst, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            \n",
    "        # Print the table format\n",
    "        print('{:^20}'.format('Minibatch'),end=\"\")\n",
    "        print('{:^20}'.format('Time'),'{:^20}'.format('Loss'),end=\"\")\n",
    "        print('{:^20}'.format('Train accuracy'),'{:^20}'.format('Validation accuracy'))\n",
    "\n",
    "        # Initialize the averages\n",
    "        batch_avg_loss = 0.0\n",
    "        batch_avg_acc = 0.0\n",
    "        batch_step = 0\n",
    "\n",
    "        # Start the session\n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            # Initialize the variables\n",
    "            tf.initialize_all_variables().run()\n",
    "            \n",
    "            # Set the logs writer to the folder /tmp/tboard/logs/\n",
    "            summary_writer = tf.train.SummaryWriter('/tmp/tboard/logs', graph=session.graph)\n",
    "\n",
    "            # Note the starting time\n",
    "            start_time = time.clock()\n",
    "            \n",
    "            # Loop through the steps\n",
    "            for step in range(num_steps):\n",
    "                # Pick an offset within the training data, which has been randomized.\n",
    "                offset = (step * self.batch_size) % (train_labels.shape[0] - self.batch_size)\n",
    "                \n",
    "                # Generate a minibatch.\n",
    "                data = train_data[offset:(offset + self.batch_size), :]\n",
    "                labels = train_labels[offset:(offset + self.batch_size), :]\n",
    "                \n",
    "                # Run the session\n",
    "                _, l, predictions, summary_str = session.run([self.opt, self.loss, self.train, self.summary],\n",
    "                                                {self.tf_x : data, self.tf_y : labels})\n",
    "                \n",
    "                # Write logs for each iteration\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                # Calculate the averages\n",
    "                if step != 0:\n",
    "                    batch_avg_loss = ((batch_avg_loss * (step - batch_step - 1)) + l) \\\n",
    "                                        / (step - batch_step)\n",
    "                    batch_avg_acc = ((batch_avg_acc * (step - batch_step - 1)) + \\\n",
    "                                     accuracy(predictions, labels)) \\\n",
    "                                        / (step - batch_step)\n",
    "                # At periodic intervals, print out the statistics\n",
    "                if ((step % (int)(num_steps / 10) == 1) or (step == num_steps - 1)): \n",
    "                    # Print the values\n",
    "                    print('{:^20}'.format(str(step)),end=\"\")\n",
    "                    print('{:^20}'.format(str(time.clock() - start_time)),end=\"\")\n",
    "                    print('{:^20}'.format(str(batch_avg_loss)),end=\"\")\n",
    "                    print('{:^20}'.format(str(batch_avg_acc)),end=\"\")\n",
    "                    print('{:^20}'.format(str(accuracy(self.cv.eval(), cv_labels))))\n",
    "                    print('{:^20}'.format(str(predictions[0])),end=\"\")\n",
    "                    print('{:^20}'.format(str(labels[0])))\n",
    "\n",
    "                    # Reset the values\n",
    "                    start_time = time.clock()\n",
    "                    batch_avg_loss = 0.0\n",
    "                    batch_avg_acc = 0.0\n",
    "                    batch_step = step\n",
    "                \n",
    "            # Print the test accuracy\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(self.tst.eval(), test_labels))\n",
    "        \n",
    "    # Print utility\n",
    "    def Print(self):\n",
    "        for entry in self.__dict__:\n",
    "            print (entry, self.__dict__[entry])\n",
    "\n",
    "# Let's create a function to randomize the data set\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\"\"\"\n",
    "Gets image data\n",
    "\"\"\"\n",
    "def GetInput(csv_path, num_img, img_sz):\n",
    "    \n",
    "    # Create empty arrays to hold the images and labels\n",
    "    data = np.ndarray(shape=(num_img, img_sz[0], img_sz[1]), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(num_img, 1), dtype=np.float32)\n",
    "    \n",
    "    # Open the CSV file and parse it\n",
    "    with open(os.path.join(csv_path,'interpolated.csv'), 'r') as csvfile:\n",
    "        cnt = 0\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in spamreader:\n",
    "            # Skip the first row\n",
    "            if row[0] != \"index\" and cnt < num_img:\n",
    "                image = cv2.imread(os.path.join(csv_path,row[5]), 0)\n",
    "                data[cnt,:,:] = cv2.resize(image, (0,0), fx=0.25, fy=0.25) \n",
    "                labels[cnt] = row[6]\n",
    "                cnt = cnt + 1\n",
    "    \n",
    "    # Let's normalize the labels\n",
    "    labels = labels - np.min(labels)\n",
    "    labels = labels / np.max(labels)\n",
    "    \n",
    "    # Return the randomized labels\n",
    "    return randomize(data, labels)\n",
    "    \n",
    "# Main function\n",
    "def main():\n",
    "    # Tuning parameters\n",
    "    IMG_SZ = [120, 160]\n",
    "    NUM_IMG = 17\n",
    "    NUM_CV = 10\n",
    "    NUM_TEST = 10\n",
    "    BATCH_SZ = 8\n",
    "    L_RATE = 0.001\n",
    "    L_DECAY_STEP = 0.1\n",
    "    L_DECAY_RATE = 0.8\n",
    "    KEEP_PROB = 1.0\n",
    "    REG_CONST = 0\n",
    "    EPOCHS = 1000\n",
    "    NN_NODES = np.array([100, 50])\n",
    "    W_STDDEV = 0.003\n",
    "    \n",
    "    # Generated constants\n",
    "    NUM_STEPS = (int)((NUM_IMG * EPOCHS) / BATCH_SZ)\n",
    "    TOT_IMG = NUM_IMG + NUM_CV + NUM_TEST\n",
    "    \n",
    "    # Get the data\n",
    "    data, labels = GetInput('/sharefolder/sdc-data/extract/', TOT_IMG, IMG_SZ)\n",
    "    \n",
    "    # Split the data into training, cross validation and testing sets\n",
    "    TrainD = np.reshape(data[:NUM_IMG,:,:], (NUM_IMG, IMG_SZ[0] * IMG_SZ[1]))\n",
    "    TrainL = np.reshape(labels[:NUM_IMG,:], (NUM_IMG, 1))\n",
    "    \n",
    "    ValidD = np.reshape(data[NUM_IMG:(NUM_IMG+NUM_CV),:,:], (NUM_CV, IMG_SZ[0] * IMG_SZ[1]))\n",
    "    ValidL = np.reshape(labels[NUM_IMG:(NUM_IMG+NUM_CV),:], (NUM_CV, 1))\n",
    "    \n",
    "    TestD = np.reshape(data[(NUM_IMG+NUM_CV):,:,:], (NUM_TEST, IMG_SZ[0] * IMG_SZ[1]))\n",
    "    TestL = np.reshape(labels[(NUM_IMG+NUM_CV):,:], (NUM_TEST, 1))\n",
    "    \n",
    "    # Initialize the NN\n",
    "    myNet = NeuralNet(BATCH_SZ, TrainD.shape[1], TrainL.shape[1])\n",
    "\n",
    "    # Define the model\n",
    "    myNet.Model(len(NN_NODES), W_STDDEV, NN_NODES, KEEP_PROB, REG_CONST, L_RATE, \n",
    "                (L_DECAY_STEP * NUM_STEPS), L_DECAY_RATE)\n",
    "\n",
    "    myNet.Train(NUM_STEPS, TrainD, TrainL, ValidD, ValidL, TestD, TestL)\n",
    "    \n",
    "    #myNet.Print()\n",
    "    \n",
    "# Script to execute the main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

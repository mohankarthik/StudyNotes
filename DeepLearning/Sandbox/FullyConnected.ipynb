{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MergeSummary/MergeSummary:0\", shape=(), dtype=string)\n",
      "     Minibatch              Time                 Loss           Train accuracy    Validation accuracy \n",
      "         1           0.027637000000027    0.0695146769285          100.0                0.0         \n",
      "        1042         6.5053949999999645   0.0692978832266          100.0                0.0         \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-f8686537b05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;31m# Script to execute the main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-f8686537b05f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m                 (_L_DECAY_STEP_ * _NUM_STEPS_), _L_DECAY_RATE_)\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mmyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NUM_STEPS_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyTrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyTrainLables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyCVData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyCVLables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyTestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyTestLables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m#myNet.Print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-f8686537b05f>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, num_steps, train_data, train_labels, cv_data, cv_labels, test_data, test_labels)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0mbatch_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_avg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m                                         \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                     \u001b[0mbatch_avg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_avg_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m                                      \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                         \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# At periodic intervals, print out the statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-f8686537b05f>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Some helper functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Let's define a simple NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Some helper functions\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "# Let's define a simple NN\n",
    "class NeuralNet:\n",
    "    ########### PRIVATE METHODS ###################\n",
    "    # Geneates a fully connected model from the input, weights and biases\n",
    "    # keeping the number of layers and the dropout probability in mind\n",
    "    def __genModel(self, x, w, b, num_layers, keep_prob):\n",
    "        # Local variable activations\n",
    "        a = dict()\n",
    "        \n",
    "        if num_layers == 0:\n",
    "            return tf.nn.bias_add(tf.matmul(x, w[0]), b[0])\n",
    "        else:\n",
    "            a[0] = tf.nn.dropout(tf.nn.relu(tf.nn.bias_add(tf.matmul(x, w[0]), b[0])), keep_prob)\n",
    "            for i in np.arange(1, num_layers):\n",
    "                a[i] = tf.nn.dropout(tf.nn.relu(tf.nn.bias_add(tf.matmul(a[i-1], w[i]), b[i])), keep_prob)\n",
    "            i = num_layers\n",
    "            return tf.nn.relu(tf.nn.bias_add(tf.matmul(a[i-1], w[i]), b[i]))\n",
    "        \n",
    "    ########### PUBLIC METHODS ###################\n",
    "    # Constructor\n",
    "    def __init__(self, batch_size, x_size, y_size):\n",
    "        # Store the values\n",
    "        self.batch_size = batch_size\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        \n",
    "        # Initialize the graph\n",
    "        self.graph = tf.Graph()\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds a configurable number of layers into the graph\n",
    "    num_layers: Number of fully connected layers to be added\n",
    "    num_nodes: A np array containing the number of nodes in each layer,\n",
    "        so num_nodes must be a list of num_layers length\n",
    "    keep_prob: The dropouts keep prob\n",
    "    reg_const: The L2 norm constant\n",
    "    init_lrate: Initial learning rate\n",
    "    lr_decay_step: Number of steps after which learning rate should decay\n",
    "    lr_decay_rate: The rate at which the learning rate should decay\n",
    "    \"\"\"\n",
    "    def Model(self, num_layers, num_nodes, keep_prob, reg_const, init_lrate, lr_decay_step, lr_decay_rate):\n",
    "        # Preconditions\n",
    "        assert num_layers == num_nodes.shape[0]\n",
    "        \n",
    "        # Store the values\n",
    "        self.num_layers = num_layers\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Initialize the w and b & activation variables\n",
    "        self.tf_w = dict()\n",
    "        self.tf_b = dict()\n",
    "        self.tb_w = dict()\n",
    "        self.tb_b = dict()\n",
    "        \n",
    "        # Form the weights and biases\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope(\"input\") as scope:\n",
    "                self.tf_x = tf.placeholder(tf.float32, shape=(self.batch_size, self.x_size))\n",
    "                self.tf_y = tf.placeholder(tf.float32, shape=(self.batch_size, self.y_size))\n",
    "                \n",
    "            # If there are no layers, connect input to output directly\n",
    "            if num_layers == 0:\n",
    "                with tf.name_scope(\"output_layer\") as scope:\n",
    "                    self.tf_w[0] = tf.Variable(tf.truncated_normal([self.x_size, self.y_size]))\n",
    "                    self.tf_b[0] = tf.Variable(tf.zeros([self.y_size]))\n",
    "            else:\n",
    "                # Add the first layer\n",
    "                with tf.name_scope(\"hidden_layer_0\") as scope:\n",
    "                    self.tf_w[0] = tf.Variable(tf.truncated_normal([self.x_size, num_nodes[0]]))\n",
    "                    self.tf_b[0] = tf.Variable(tf.zeros([num_nodes[0]]))\n",
    "\n",
    "                # Add the intermediate layers\n",
    "                for i in np.arange(1, num_layers):\n",
    "                    with tf.name_scope(\"hidden_layer_\"+str(i)) as scope:\n",
    "                        self.tf_w[i] = tf.Variable(tf.truncated_normal([num_nodes[i-1], num_nodes[i]]))\n",
    "                        self.tf_b[i] = tf.Variable(tf.zeros([num_nodes[i]]))\n",
    "\n",
    "                # Connect to the output layer\n",
    "                with tf.name_scope(\"output_layer\") as scope:\n",
    "                    self.tf_w[num_layers] = tf.Variable(tf.truncated_normal([num_nodes[num_layers-1], self.y_size]))\n",
    "                    self.tf_b[num_layers] = tf.Variable(tf.zeros([self.y_size]))\n",
    "            \n",
    "            # Add summary ops to collect data\n",
    "            for i in np.arange(num_layers + 1):\n",
    "                self.tb_w[i] = tf.histogram_summary(\"weights\"+str(i), self.tf_w[i])\n",
    "                self.tb_b[i] = tf.histogram_summary(\"biases\"+str(i), self.tf_b[i])\n",
    "            \n",
    "            # Model the training, evaluation, cross validation and testing\n",
    "            self.logits = self.__genModel(self.tf_x, self.tf_w, self.tf_b, self.num_layers, keep_prob)\n",
    "            self.train = self.__genModel(self.tf_x, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            \n",
    "            # Implement the loss function\n",
    "            with tf.name_scope(\"cost_function\") as scope:\n",
    "                # Loss\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits, self.tf_y))\n",
    "\n",
    "                # L2 regularization\n",
    "                reg = 0\n",
    "                for i in np.arange(self.num_layers + 1):\n",
    "                    reg += tf.nn.l2_loss(self.tf_w[i]) + tf.nn.l2_loss(self.tf_b[i])\n",
    "                reg *= reg_const\n",
    "\n",
    "                # Add the regularization to the loss\n",
    "                self.loss = tf.reduce_mean(self.loss + reg)\n",
    "\n",
    "                # Create a summary to monitor the cost function\n",
    "                tf.scalar_summary(\"cost_function\", self.loss)\n",
    "\n",
    "            # Create the global step variable\n",
    "            self.gstep = 0\n",
    "\n",
    "            # Implement the optimizer\n",
    "            with tf.name_scope(\"back_prop\") as scope:\n",
    "                # Optimizer with a decaying learning rate\n",
    "                self.lrate = tf.train.exponential_decay(init_lrate, self.gstep, \n",
    "                                                        lr_decay_step, lr_decay_rate, staircase=True)\n",
    "                self.opt = tf.train.GradientDescentOptimizer(self.lrate).minimize(self.loss)\n",
    "\n",
    "                # Create a summary to monitor the cost function\n",
    "                tf.scalar_summary(\"learning_rate\", self.lrate)\n",
    "            \n",
    "            # Merge all the summaries and start the summary writer\n",
    "            self.summary = tf.merge_all_summaries()\n",
    "            print (self.summary)\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the model. This should be invoked only after the model is formed\n",
    "    \n",
    "    \"\"\"\n",
    "    def Train(self, num_steps, train_data, train_labels, cv_data, cv_labels, test_data, test_labels):\n",
    "        with self.graph.as_default():\n",
    "            # Add the test and cross validation data sets\n",
    "            self.tf_cv = tf.constant(cv_data)\n",
    "            self.tf_tst = tf.constant(test_data)\n",
    "\n",
    "            # Set up the evaluations for the CV and test sets\n",
    "            self.cv = self.__genModel(self.tf_cv, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            self.tst = self.__genModel(self.tf_tst, self.tf_w, self.tf_b, self.num_layers, 1.0)\n",
    "            \n",
    "        # Print the table format\n",
    "        print('{:^20}'.format('Minibatch'),end=\"\")\n",
    "        print('{:^20}'.format('Time'),'{:^20}'.format('Loss'),end=\"\")\n",
    "        print('{:^20}'.format('Train accuracy'),'{:^20}'.format('Validation accuracy'))\n",
    "\n",
    "        # Initialize the averages\n",
    "        batch_avg_loss = 0.0\n",
    "        batch_avg_acc = 0.0\n",
    "        batch_step = 0\n",
    "\n",
    "        # Start the session\n",
    "        with tf.Session(graph=self.graph) as session:\n",
    "            # Initialize the variables\n",
    "            tf.initialize_all_variables().run()\n",
    "            \n",
    "            # Set the logs writer to the folder /tmp/tboard/logs/\n",
    "            summary_writer = tf.train.SummaryWriter('/tmp/tboard/logs', graph=session.graph)\n",
    "\n",
    "            # Note the starting time\n",
    "            start_time = time.clock()\n",
    "            \n",
    "            # Loop through the steps\n",
    "            for step in range(num_steps):\n",
    "                # Pick an offset within the training data, which has been randomized.\n",
    "                offset = (step * self.batch_size) % (train_labels.shape[0] - self.batch_size)\n",
    "                \n",
    "                # Generate a minibatch.\n",
    "                data = train_data[offset:(offset + self.batch_size), :]\n",
    "                labels = train_labels[offset:(offset + self.batch_size), :]\n",
    "                \n",
    "                # Run the session\n",
    "                _, l, predictions, summary_str = session.run([self.opt, self.loss, self.train, self.summary],\n",
    "                                                {self.tf_x : data, self.tf_y : labels})\n",
    "                \n",
    "                # Write logs for each iteration\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                # Calculate the averages\n",
    "                if step != 0:\n",
    "                    batch_avg_loss = ((batch_avg_loss * (step - batch_step - 1)) + l) \\\n",
    "                                        / (step - batch_step)\n",
    "                    batch_avg_acc = ((batch_avg_acc * (step - batch_step - 1)) + \\\n",
    "                                     accuracy(predictions, labels)) \\\n",
    "                                        / (step - batch_step)\n",
    "                # At periodic intervals, print out the statistics\n",
    "                if ((step % (int)(num_steps / 15) == 1) or (step == num_steps - 1)): \n",
    "                    # Print the values\n",
    "                    print('{:^20}'.format(str(step)),end=\"\")\n",
    "                    print('{:^20}'.format(str(time.clock() - start_time)),end=\"\")\n",
    "                    print('{:^20}'.format(str(batch_avg_loss)),end=\"\")\n",
    "                    print('{:^20}'.format(str(batch_avg_acc)),end=\"\")\n",
    "                    print('{:^20}'.format(str(accuracy(self.cv.eval(), cv_labels))))\n",
    "\n",
    "                    # Reset the values\n",
    "                    start_time = time.clock()\n",
    "                    batch_avg_loss = 0.0\n",
    "                    batch_avg_acc = 0.0\n",
    "                    batch_step = step\n",
    "                \n",
    "            # Print the test accuracy\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(self.tst.eval(), test_labels))\n",
    "        \n",
    "    # Print utility\n",
    "    def Print(self):\n",
    "        for entry in self.__dict__:\n",
    "            print (entry, self.__dict__[entry])\n",
    "            \n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Get the data\n",
    "    myTrainData = np.random.random((200000, 100))\n",
    "    myTrainLables = np.zeros((200000, 10), dtype=np.float32)\n",
    "    for i in np.arange(200000):\n",
    "        myTrainLables[i][8] = 1.0\n",
    "    myCVData = np.random.random((100, 100)).astype(np.float32)\n",
    "    myCVLables = np.zeros((100, 10), dtype=np.float32)\n",
    "    for i in np.arange(100):\n",
    "        myCVLables[i][7] = 1.0\n",
    "    myTestData = np.random.random((100, 100)).astype(np.float32)\n",
    "    myTestLables = np.zeros((100, 10), dtype=np.float32)\n",
    "    for i in np.arange(100):\n",
    "        myTestLables[i][8] = 1.0\n",
    "    \n",
    "    _BATCH_SZ_ = 128\n",
    "    _L_RATE_ = 0.3\n",
    "    _L_DECAY_STEP_ = 0.1\n",
    "    _L_DECAY_RATE_ = 0.8\n",
    "    _KEEP_PROB_ = 0.5\n",
    "    _REG_CONST_ = 1e-5\n",
    "    _EPOCHS_ = 10\n",
    "    _NUM_STEPS_ = (int)((myTrainData.shape[0] * _EPOCHS_) / _BATCH_SZ_)\n",
    "    \n",
    "    # Initialize the NN\n",
    "    myNet = NeuralNet(\n",
    "        _BATCH_SZ_, myTrainData.shape[1], myTrainLables.shape[1])\n",
    "    \n",
    "    # Define the model\n",
    "    myNet.Model(2, np.array([100, 50]), _KEEP_PROB_, _REG_CONST_, _L_RATE_, \n",
    "                (_L_DECAY_STEP_ * _NUM_STEPS_), _L_DECAY_RATE_)\n",
    "    \n",
    "    myNet.Train(_NUM_STEPS_, myTrainData, myTrainLables, myCVData, myCVLables, myTestData, myTestLables)\n",
    "    \n",
    "    #myNet.Print()\n",
    "    \n",
    "# Script to execute the main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

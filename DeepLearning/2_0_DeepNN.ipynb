{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Nets\n",
    "## Non Linearity\n",
    "Linear models cannot model non-linear systems, and most practical systems are non-linear. To introduce non-linearity, we can add higher dimension values of each parameter, but this is too expensive.\n",
    "\n",
    "## Rectified Linear Units\n",
    "![Relu](img/Relu.png)\n",
    "\n",
    "A simple non linear function\n",
    "![Simple NN](img/BasicNNUnit.png)\n",
    "\n",
    "$$Y = W_{2}(RELU(W_{1}x + b_{1})) + b_{2}$$\n",
    "This becomes the first 2 layer neural network where the first set of weights and biases become the input layer and the second set of weights and biases becomes the hidden layer. The Relu is part of the input layer\n",
    "\n",
    "**Intuition:** https://www.academia.edu/7826776/Mathematical_Intuition_for_Performance_of_Rectified_Linear_Unit_in_Deep_Neural_Networks\n",
    "\n",
    "\n",
    "## Back Prop\n",
    "![NN Back Prop](img/NNBackProp.png)\n",
    "\n",
    "**Truncated Normal Distribution**\n",
    "The weights of this above neural net should be initialized over a truncated normal distribution, \n",
    "Which is the probability distribution of a normally distributed random variable whose value is either bounded below or above (or both)\n",
    "\n",
    "**Deep vs wide networks**\n",
    "* To achieve better results, we can either increase the number of neurons in each layer, or add more layers\n",
    "* Going deeper is generally the best approach both for performance as well as compute efficiency\n",
    "* Most practical things are hierarchically oriented, a deep network is able to pick out these components at each layer. (Check visualization in Resources). So the nets architecture can be visually picked out to form the major features that we as humans expect to see.\n",
    "![Deep NN](img/DeepNN.png)\n",
    "\n",
    "## Regularization\n",
    "Three approaches are there\n",
    "1) Early termination is the best approach, check the validation set accuracy and stop when it starts to drop\n",
    "\t\n",
    "2) The other approach is to use L2 Regularization\n",
    "\t\n",
    "\tWhere we add a L2 norm of the weights multiplied by a constant \"BETA\" to ensure that the weights are kept to a small value.\n",
    "\t\n",
    "3) Dropouts\n",
    "Oh FUCK… OK so randomly drop out half the activations going from one layer to the next, every sample. This means that the net has to always depend on all the activations rather than just focus on a few. It's like whack a mole.\n",
    "Dropouts are the most important aspect in regularization to emerge in the recent years… If dropout doesn't work, then the network architecture is wrong.\n",
    "\tOne important implementation trick is to scale the outputs during training so that when we get only 2 valid output activations, we scale them up (multiply by 2) and then take an average of all the activations.. During eval, we don't do dropouts…\n",
    "\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "* It's similar to supervised learning, where we are given a bunch of x and y's and we need to find a function that maps the x's to the y's.\n",
    "* But in reinformcement learning, we are given x's and z's!!, and we should find a function that maps x's to y's. Brilliant. Makes perfect sense right!!\n",
    "\n",
    "## Markov Decision Process\n",
    "![](img/TheWorld.png)\n",
    "* Consider the above world, where we need to go from start to the green box without touching the red box.\n",
    "* Say there are uncertainities that when we execute an action (say going up), it'll be successful only some percentage of the time and in other cases we might go in other directions or not move at all.\n",
    "* We can model such a world using Markovian properties. To get started\n",
    "\n",
    "### Terminologies\n",
    "* Markovian Property 1: **Only the present matters**\n",
    "  * Quoting Oogway: *Quit. Don't quit. Noodles. Don't noodles. You are too concerned with what was and what will be. There's a saying: \"Yesterday is history, tomorrow is a mystery, but today is a gift. That is why it is called a \"present\"*\n",
    "  * When we consider decisions, we only need to worry about the present state and not about our previous state(s). In other words, it does not matter how we got here, it only matters that we are.\n",
    "* Markovian Property 2: **The rules don't change**\n",
    "* **Agent**: The player\n",
    "* **State (s)**: A description of the agent's current. In the case of the above world, it'll probably be the position on the grid.\n",
    "  * In case a problem needs to remember some of the past, then the current state could keep a trace of the recent history.\n",
    "  * If we need to remember everything from the begining, then the state would be very large, and we'll see each state only once. So we can't learn anything much. But in practical cases of short term memory, we can.\n",
    "* **Action (a(s)/a)**: The actions that are possible in each state. In the above case, it'll be up, down, left and right. Sometimes actions depend on the state you are in, such as some actions are possible in some states and not in others, etc...\n",
    "* **Model / Transition T(s, a, s')**: This describes the rules of the game / physics of the world. The model is a function of the state, the action taken, and a destination state. The model then gives the probability P(S' | S,A); i.e: the probability we'll land in S' given that we were in state S and took action A.\n",
    "  * Note that we are only looking at the current S and not the previous states.\n",
    "  * So for each S,A, if we sum up all the probabilities of landing in each of the S', we should get 1.\n",
    "* **Reward (r(s), r(s,a), r(s,a,s'))**: It's a scalar value that's assigned to a state, (and a action and destination state). In the above world, the reward to reaching the green square could be positive, and the reward for reaching the red square could be negative.\n",
    "* **Policy (Pi(s) = a)**: The policy is the solution to the MDP problem defined by the model. It gives us a action for each of the state that we are in. There is a policy pi\\*, that'll be the optimum solution of all the policies possible.\n",
    "\n",
    "Note:\n",
    "1) In a supervised learning, we'll be given state and the action taken and allow our machine to learn the actions for each state.\n",
    "2) Here we are given the state, the action and the reward (which is the z).\n",
    "  * Note that in some cases the rewards can come very later (in the above example, we get a reward only if we reach the green square).\n",
    "  \n",
    "## Rewards\n",
    "### Delayed Rewards\n",
    "* We take a sequence of actions and *maybe* we get a reward after a series of actions...\n",
    "* This makes evaluating actions very difficult and it's called the **Credit Assignment Problem**\n",
    "  * In this case it's a temporal credit assignment problem\n",
    "\n",
    "So how do we give constant rewards?\n",
    "* In the above world, say every state except the green and red have a -0.04 reward.\n",
    "  * This is like a slightly hot beach, that you've to walk across fast to reach our destination to get the 1.\n",
    "  * This will encourage our system to quickyl finish the game by reaching the green\n",
    "* In this world the optimum policy might be\n",
    "![](img/OptimumPolicy.png)\n",
    "  * In the bottom, we take the long way around because, it keeps us away from the big bad negative one. ALthough we might incur some more -0.04, but the cost is still less than going towards the negative one and by chance falling into the pit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "## Gradient Descent\n",
    "1. Let X[n,m] be the set of known inputs; where n is the number of features and m is the number of training samples\n",
    "  1. X[i,j] is denoted as $$x_{i}^j$$; where i is the feature number and j is the training sample.\n",
    "  2. Features are the number of various parameters that affect the output\n",
    "  3. Features should not be linearly related to each other. For ex., $$x_{i}^k \\neq C_{1} + C_{2} * x_{j}^k \\forall (i,j,k)$$; where C<sub>1</sub> and C<sub>2</sub> are some arbitrary constants\n",
    "  4. Features can be non-linearly related to each other, for ex., $$x_{i}^k= (x_{j}^k)^2 \\forall (i,j,k)$$\n",
    "  5. All features should be in the *same scale*, for ex.,$$\\bar(x_{i}) \\approx \\bar(x_{j}) \\forall (i,j)$$ $$range(x_{i}) \\approx range(X_{j}) \\forall (i,j)$$\n",
    "2. Let Y[m] be the set of known outputs\n",
    "3. The hypothesis $$h_{\\theta}(x)= \\theta^{T}X =\\theta_{0}+ \\theta_{1}x_{1}+…$$ where θ is the prediction paramaters. We want h<sub>Θ</sub>(x) to be as close to Y as possible.\n",
    "  1. This is a linear fit. (see image below)\n",
    "  2. By choosing some x<sub>i</sub> to be the square or cube of a previous x, we can make the fit quadratic or cubic as well.\n",
    "![Linear Regression Fit Image](img/LinRegFit.png)\n",
    "4. The cost function $$J(θ)= \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{i}) - y^{i})^2$$\n",
    "5. To reduce this cost function; gradient descent will iteratively reduce J by updating such that J becomes as close to 0 as possible.\n",
    "6. The simultaneous update function is as follows $$\\theta_{j} := \\theta_{j} − \\alpha\\frac{1}{m}\\sum_{i=1}(h_{\\theta}(x^{i}) − y^i)x_{j}^i$$ Simultaneously update all θ<sub>j</sub> for all j. This results in the following:\n",
    "![Gradient Descent](img/GradDesc.png)\n",
    "\t\n",
    "7. Run for enough iterations until θ does not differ by much (until θ converges)\n",
    "\n",
    "## Normal Equation\n",
    "1. For cost functions where n is small, there is a direct way to get the final answer without iterating. The equation for the optimal θ is as follows$$\\theta = (X^{T} * X)^{−1} * X^{T} * \\vec(y)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "## Motivation\n",
    "* We may have to work with lots of features for most real world problems:\n",
    "  * Say 100 original features. If we want to include quadratic, cubic or higher polynomial features, then grows incredibly high.\n",
    "  * Say for 100 original features, including up to 6th order polynomials, we get around 10^8 features.\n",
    "  * This will be prohibitively difficult to implement using linear or logistic regression.\n",
    "  * Neural nets give a unique way of machine learning that might outset the problem of large number of features.\n",
    "\t\t\n",
    "## Model Representation\n",
    "![Neural Network](img/NeuralNet.png)\n",
    "### Terminologies\n",
    "$$ a_{i}^j  \\textrm{−\"\"activation\" of unit \"i in layer j}$$\n",
    "$$\\theta_{j} \\textrm{- matrix of weights controlling function mapping from layer j to j + 1}$$\n",
    "\n",
    "Hence\n",
    "$$a_{1}^{(2)}=g(\\Theta_{10}^{(1)}x_{0}+\\Theta_{11}^{(1)}x_{1}+\\Theta_{12}^{(1)}x_{2}+\\Theta_{13}^{(1)}x_{3})=g(z_{1}^{(2)})$$\n",
    "$$a_{2}^{(2)}=g(\\Theta_{20}^{(1)}x_{0}+\\Theta_{21}^{(1)}x_{1}+\\Theta_{22}^{(1)}x_{2}+\\Theta_{23}^{(1)}x_{3})=g(z_{2}^{(2)})$$\n",
    "$$a_{3}^{(2)}=g(\\Theta_{30}^{(1)}x_{0}+\\Theta_{31}^{(1)}x_{1}+\\Theta_{32}^{(1)}x_{2}+\\Theta_{33}^{(1)}x_{3})=g(z_{3}^{(2)})$$\n",
    "$$h_{\\Theta}(x)=a_{1}^{(3)}=g(\\Theta_{10}^{(2)}a_{0}^{(2)}+\\Theta_{11}^{(2)}a_{1}^{(2)}+\\Theta_{12}^{(2)}a_{2}^{(2)}+\\Theta_{13}^{(2)} a_{3}^{(2)})=g(z_{1}^{(3)})$$\n",
    "$$\\Theta_{j} \\textrm{ therefore is a } s_{j+1} * s_{j} + 1 \\textrm{ dimension matrix where } s_{j} \\textrm{ is the number of units in layer j}$$\n",
    "\t\n",
    "### Vectorized implementation (forward propagation)\n",
    "$$x_{0}=1$$\n",
    "$$z^{(2)}=\\Theta^{(1)} * X$$\n",
    "$$a^{(2)}=g(z^{(2)})$$\n",
    "\n",
    "$$a_{0}^{(2)}=1$$\n",
    "$$z^{(3)}=\\Theta^{(2)} * X$$\n",
    "$$a^{(3)}=g(z^{(3)})$$\n",
    "\n",
    "and so on…\n",
    "\n",
    "## MultiClass Classification\n",
    "![Multiclass classification](img/NeuralNetClassification.png)\n",
    "\n",
    "http://blog.davidsingleton.org/nnrccar/\n",
    "\n",
    "### Training: Cost function\n",
    "This will be a generalization of the logistic regression\n",
    "\n",
    "$$J(\\Theta)=−\\frac{1}{m}\\Big[\\sum_{i=1}^m\\sum_{k=1}^Ky_{k}^{(i)}\\log\\big(h_{\\Theta}(x^{(i)})_{k}\\big)+(1−y_{k}^{(i)})\\log\\Big(1−\\big(h_{\\Theta}(x^{(i)})\\big)_{k}\\Big)\\Big]+\\frac{λ}{2m} \\sum_{l=1}^L\\sum_{i=1}^{s_{l}}\\sum_{j=1}^{s_{l+1}}\\big(\\Theta_{ij}^l\\big)^2$$\n",
    "Where\n",
    "* K is the total number of outputs; 1 for binary classification and ≥3 for multiclass classification\n",
    "* L is the total number of layers in the neural net\n",
    "* s<sub>l</sub>  is the total number of neurons in layer l\n",
    "* h<sub>Θ</sub>(x) ∈ R<sup>K</sup>\n",
    "* (h<sub>Θ</sub>(x<sup>(i)</sup>)<sub>j</sub>=j<sup>th</sup> hypothesis of neural net for the i<sup>th</sup> training set\n",
    "* y<sub>k</sub><sup>(i)</sup> = k<sup>th</sup> output of the neural net for the i<sup>th</sup> training input\n",
    "\n",
    "### Training: Back propagation\n",
    "$$\\textrm{Let }\\delta_{j}^{(l)} \\textrm{ = \"\"error\" of node j in layer\" l}$$\n",
    "Therefore\n",
    "$$\\delta^{(4)}=a^{(4)}−y$$\n",
    "\n",
    "For the other layers, the error is calculated as\n",
    "$$\\delta^{(l)}=(\\Theta^{(l)})^{T}\\delta^{(l+1)}∗g′(z^{(l)})$$\n",
    "\n",
    "Each of these delta terms are basically a partial derivative of the cost at that particular point. And the intuition here is to calculate the gradient of the cost to find out in which direction each of these weights will have to slide so as to minimize the cost at each and every node. The g-prime component is basically the partial derivative of the hypothesis function. Given the use of sigmoid function, the following is it's derivative.\n",
    "$$g′(z^{(l)})=a^{(l)}∗(1−a^{(l)})$$\n",
    "\n",
    "Note: There is no delta1. Because that would signify the error in the input, which cannot exist.\n",
    "\n",
    "### Random Initialization\n",
    "* 0 init or same init of theta does not work for neural nets because\n",
    "  * Because giving the same weight for all theta, will make all the activations will compute the same values\n",
    "  * This will also imply that the delta (errors) in back prop will also be the same\n",
    "  * So basically we will cut down the neural net to just one neuron per layer essentially.\n",
    "* To break symmetry, initialize each Θ<sub>ij</sub><sup>(l)</sup> to a random value in [−ε,ε]\n",
    "\n",
    "## Architecture\n",
    "1. Chose the number of inputs (based on number of features)\n",
    "2. Choose the number of outputs (classes)\n",
    "3. Choose the number of layers (typically 1 hidden) and number of neurons for each layer\n",
    "  * If more than 1 hidden layer is used, typically all hidden layers have the same number of neurons\n",
    "  * With respect to number of hidden neurons, the more the better. Have to weigh out performance\n",
    "  * It is typically comparable to the number of input features\n",
    "4. Randomly initialize the weights\n",
    "5. Loop for all inputs\n",
    "  * Implement forward prop and find h<sub>Θ</sub>(x<sup>(i)</sup>) and all a<sub>ij</sub><sup>(l)</sup> for any x<sup>(i)</sup>\n",
    "  * Implement code to complete cost function J(Θ)\n",
    "  * Implement back prop to compute partial derivatives of J(Θ) with respect to Θ<sub>jk</sub><sup>(l)</sup> and get δ<sup>(l)</sup>\n",
    "6. Compute the D terms including the regularization terms.\n",
    "7. (Optionally during debug) use gradient checking to ensure that the D terms are correct\n",
    "8. Use an advanced optimization method to minimize J(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
